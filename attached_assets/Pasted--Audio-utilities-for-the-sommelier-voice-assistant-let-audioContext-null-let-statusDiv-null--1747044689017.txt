// Audio utilities for the sommelier voice assistant
let audioContext = null;
let statusDiv = null;
let lastInputWasVoice = false;
let audioDebugDiv = null;
let currentAudio = null;
let volume = 1.0;

// Initialize everything when the DOM is ready
document.addEventListener('DOMContentLoaded', function() {
  statusDiv = document.getElementById('status');
  audioDebugDiv = document.getElementById('audio-debug');
  
  // Initialize volume control
  const volumeControl = document.getElementById('volume-control');
  if (volumeControl) {
    volumeControl.addEventListener('input', function() {
      volume = parseFloat(this.value);
      updateDebugStatus(`Volume set to ${volume}`);
      
      // If there's currently playing audio, update its volume
      if (currentAudio) {
        currentAudio.volume = volume;
      }
    });
  }
  
  // Setup test buttons
  const testBrowserSpeech = document.getElementById('test-browser-speech');
  if (testBrowserSpeech) {
    testBrowserSpeech.addEventListener('click', function() {
      updateDebugStatus('Testing browser speech synthesis...');
      const utterance = new SpeechSynthesisUtterance('Hello, I am the sommelier voice assistant. Can you hear me?');
      utterance.volume = volume;
      utterance.onend = () => updateDebugStatus('Browser speech test complete');
      utterance.onerror = (e) => updateDebugStatus(`Speech error: ${e.error}`);
      window.speechSynthesis.speak(utterance);
    });
  }
  
  // Initialize audio context with user interaction
  const testAudioContext = document.getElementById('test-audio-context');
  if (testAudioContext) {
    testAudioContext.addEventListener('click', function() {
      initAudioContext(true);
    });
  }
  
  // Speech recognition setup for microphone input
  setupSpeechRecognition();
  
  // Setup message observer to auto-speak responses
  setupMessageObserver();
  
  // Try to initialize audio
  initAudioContext(false);
  
  updateDebugStatus('Audio system initialized');
});

// Initialize audio context with optional force flag
function initAudioContext(force) {
  if (!audioContext || force) {
    try {
      const AudioContext = window.AudioContext || window.webkitAudioContext;
      audioContext = new AudioContext();
      
      // Resume the context (needed for newer browsers)
      audioContext.resume().then(() => {
        updateDebugStatus(`Audio context initialized and resumed. State: ${audioContext.state}`);
        
        // Create and play a silent sound to fully activate audio
        const silentOscillator = audioContext.createOscillator();
        const gainNode = audioContext.createGain();
        gainNode.gain.value = 0.001; // Virtually silent
        silentOscillator.connect(gainNode);
        gainNode.connect(audioContext.destination);
        silentOscillator.start();
        silentOscillator.stop(audioContext.currentTime + 0.001);
        
        updateDebugStatus('Audio system fully activated');
      }).catch(e => {
        updateDebugStatus(`Failed to resume audio context: ${e.message}`);
      });
    } catch (e) {
      updateDebugStatus(`Audio context creation failed: ${e.message}`);
    }
  } else if (audioContext.state === 'suspended') {
    audioContext.resume().then(() => {
      updateDebugStatus(`Audio context resumed. State: ${audioContext.state}`);
    });
  }
}

// Set up speech recognition for microphone input
function setupSpeechRecognition() {
  const micButton = document.getElementById('mic-button');
  if (!micButton) return;
  
  let recognition = null;
  if (window.SpeechRecognition || window.webkitSpeechRecognition) {
    recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = 'en-US';
    recognition.continuous = false;
    recognition.interimResults = false;
    
    recognition.onresult = function(event) {
      const transcript = event.results[0][0].transcript;
      if (statusDiv) statusDiv.textContent = 'Processing your question...';
      updateDebugStatus(`Voice input: "${transcript}"`);
      
      // Set the flag to indicate voice input
      lastInputWasVoice = true;
      
      // Ensure audio is ready
      initAudioContext(false);
      
      // Send the message through your chat interface
      sendMessage(transcript);
    };
    
    recognition.onend = function() {
      micButton.classList.remove('listening');
      if (statusDiv) statusDiv.textContent = '';
    };
    
    recognition.onstart = function() {
      micButton.classList.add('listening');
      if (statusDiv) statusDiv.textContent = 'Listening for your question...';
      updateDebugStatus('Listening for voice input...');
    };
    
    recognition.onerror = function(event) {
      console.error('Speech recognition error:', event.error);
      if (statusDiv) statusDiv.textContent = `Error: ${event.error}`;
      micButton.classList.remove('listening');
      updateDebugStatus(`Recognition error: ${event.error}`);
    };
    
    micButton.addEventListener('click', function() {
      initAudioContext(true); // Force audio context initialization on mic click
      
      if (micButton.classList.contains('listening')) {
        recognition.stop();
      } else {
        try {
          recognition.start();
        } catch (error) {
          console.error('Failed to start speech recognition:', error);
          updateDebugStatus(`Failed to start recognition: ${error.message}`);
        }
      }
    });
  } else {
    micButton.disabled = true;
    updateDebugStatus('Speech recognition not supported in this browser');
  }
}

// Observer to detect new messages and auto-speak them
function setupMessageObserver() {
  const conversationElement = document.getElementById('conversation');
  if (!conversationElement) return;
  
  const messageObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (mutation.type === 'childList' && mutation.addedNodes.length > 0) {
        // Check if we should auto-speak
        for (const node of mutation.addedNodes) {
          if (node.nodeType === 1 && // Element node
              node.classList.contains('assistant') && 
              node.getAttribute('data-role') === 'assistant' && 
              !node.hasAttribute('data-spoken') && 
              lastInputWasVoice) {
            
            // Mark as spoken and delay slightly to ensure DOM is complete
            node.setAttribute('data-spoken', 'true');
            const messageText = node.textContent || '';
            
            setTimeout(() => {
              updateDebugStatus(`Auto-speaking: "${messageText.substring(0, 30)}..."`);
              playResponse(messageText);
            }, 300);
            
            // Reset the flag after handling
            lastInputWasVoice = false;
            break;
          }
        }
      }
    }
  });
  
  // Start observing the conversation for changes
  messageObserver.observe(conversationElement, { 
    childList: true, 
    subtree: true 
  });
  
  updateDebugStatus('Message observer initialized');
}

// Main function to play the voice response
async function playResponse(text) {
  try {
    if (statusDiv) statusDiv.textContent = 'Getting voice response...';
    updateDebugStatus('Preparing to speak response...');
    
    // Try different methods to play audio, starting with OpenAI TTS
    await playWithOpenAITTS(text)
      .catch(async e => {
        updateDebugStatus(`OpenAI TTS failed: ${e.message}, trying browser speech`);
        await playWithBrowserSpeech(text)
          .catch(e => {
            updateDebugStatus(`Browser speech failed: ${e.message}, no audio available`);
            if (statusDiv) statusDiv.textContent = 'Audio playback failed';
          });
      });
  } catch (error) {
    console.error("Error in playResponse:", error);
    updateDebugStatus(`Playback error: ${error.message}`);
  }
}

// Try to play using OpenAI's Text-to-Speech API
async function playWithOpenAITTS(text) {
  updateDebugStatus('Fetching audio from OpenAI TTS...');
  
  const response = await fetch('/api/text-to-speech', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text })
  });
  
  if (!response.ok) {
    throw new Error(`HTTP error: ${response.status}`);
  }
  
  const blob = await response.blob();
  updateDebugStatus(`Audio received: ${blob.size} bytes, type: ${blob.type}`);
  
  // Create and configure audio element
  if (currentAudio) {
    currentAudio.pause();
    currentAudio.src = '';
    if (currentAudio.parentNode) {
      currentAudio.parentNode.removeChild(currentAudio);
    }
  }
  
  currentAudio = new Audio();
  currentAudio.id = 'current-audio';
  currentAudio.volume = volume;
  document.body.appendChild(currentAudio);
  
  // Set up event listeners
  currentAudio.addEventListener('canplaythrough', () => {
    updateDebugStatus('Audio ready to play');
  });
  
  currentAudio.addEventListener('playing', () => {
    if (statusDiv) statusDiv.textContent = 'Speaking...';
    updateDebugStatus('Audio playback started');
  });
  
  currentAudio.addEventListener('ended', () => {
    if (statusDiv) statusDiv.textContent = '';
    updateDebugStatus('Audio playback completed');
  });
  
  currentAudio.addEventListener('error', (e) => {
    updateDebugStatus(`Audio playback error: ${e.error}`);
    throw new Error('Audio playback error');
  });
  
  // Set source and play
  const url = URL.createObjectURL(blob);
  currentAudio.src = url;
  
  try {
    // Show play button for manual trigger if needed
    showPlayButton();
    
    // Try to autoplay
    await currentAudio.play();
    updateDebugStatus('Autoplay successful');
    
    // Clean up when playback ends
    currentAudio.onended = () => {
      URL.revokeObjectURL(url);
      if (statusDiv) statusDiv.textContent = '';
    };
  } catch (playError) {
    updateDebugStatus(`Autoplay failed: ${playError.message}. Try clicking the play button.`);
    throw new Error('Autoplay blocked by browser');
  }
}

// Fallback to browser's built-in speech synthesis
async function playWithBrowserSpeech(text) {
  return new Promise((resolve, reject) => {
    if (!('speechSynthesis' in window)) {
      reject(new Error('Browser speech synthesis not supported'));
      return;
    }
    
    updateDebugStatus('Using browser speech synthesis...');
    
    // Cancel any current speech
    window.speechSynthesis.cancel();
    
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'en-US';
    utterance.volume = volume;
    
    utterance.onend = () => {
      if (statusDiv) statusDiv.textContent = '';
      updateDebugStatus('Browser speech complete');
      resolve();
    };
    
    utterance.onerror = (e) => {
      updateDebugStatus(`Browser speech error: ${e.error}`);
      reject(new Error(`Speech synthesis error: ${e.error}`));
    };
    
    if (statusDiv) statusDiv.textContent = 'Speaking (browser synthesis)...';
    window.speechSynthesis.speak(utterance);
  });
}

// Helper function to update the debug status
function updateDebugStatus(message) {
  console.log(message);
  if (audioDebugDiv) {
    audioDebugDiv.textContent = message;
  }
}

// Show playback button for manual control
function showPlayButton() {
  const audioControls = document.getElementById('audio-controls');
  if (!audioControls) return;
  
  audioControls.style.display = 'block';
  
  const playBtn = document.getElementById('play-audio-btn');
  if (!playBtn) return;
  
  // Replace with fresh button to remove old event listeners
  const newPlayBtn = playBtn.cloneNode(true);
  playBtn.parentNode.replaceChild(newPlayBtn, playBtn);
  
  // Style button
  newPlayBtn.style.backgroundColor = '#8B0000';
  newPlayBtn.style.color = 'white';
  newPlayBtn.style.padding = '10px 20px';
  newPlayBtn.style.border = 'none';
  newPlayBtn.style.borderRadius = '5px';
  newPlayBtn.style.cursor = 'pointer';
  newPlayBtn.style.fontWeight = 'bold';
  
  // Add click listener
  newPlayBtn.addEventListener('click', function() {
    updateDebugStatus('Manual play button clicked');
    if (currentAudio) {
      currentAudio.volume = volume;
      currentAudio.play()
        .then(() => {
          if (statusDiv) statusDiv.textContent = 'Playing...';
        })
        .catch(err => {
          updateDebugStatus(`Manual play failed: ${err.message}`);
          
          // Try browser speech as fallback
          playWithBrowserSpeech(currentAudio.dataset.text || 'Unable to play audio')
            .catch(e => updateDebugStatus(`All playback methods failed: ${e.message}`));
        });
    } else {
      updateDebugStatus('No audio available to play');
    }
  });
}

// Export functions for use in React components if needed
if (typeof window !== 'undefined') {
  window.voiceAssistant = {
    playResponse,
    initAudioContext
  };
}