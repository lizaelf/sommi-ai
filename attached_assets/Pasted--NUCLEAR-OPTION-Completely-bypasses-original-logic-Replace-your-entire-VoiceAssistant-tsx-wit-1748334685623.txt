// NUCLEAR OPTION - Completely bypasses original logic
// Replace your entire VoiceAssistant.tsx with this minimal version:

import React, { useState, useRef, useEffect } from 'react';
import { useToast } from '@/hooks/use-toast';
import VoiceBottomSheet from './VoiceBottomSheet';

interface VoiceAssistantProps {
  onSendMessage: (message: string) => void;
  isProcessing: boolean;
}

const VoiceAssistant: React.FC<VoiceAssistantProps> = ({ onSendMessage, isProcessing }) => {
  const [isListening, setIsListening] = useState(false);
  const [showBottomSheet, setShowBottomSheet] = useState(false);
  const [hasAskedQuestion, setHasAskedQuestion] = useState(false);
  
  // NUCLEAR: Super simple state - only 3 modes
  const [mode, setMode] = useState<'idle' | 'thinking' | 'suggestions'>('idle');
  
  const recognitionRef = useRef<any>(null);
  const nuclearTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const { toast } = useToast();

  // NUCLEAR: Always exit thinking mode after 3 seconds
  const startNuclearTimeout = () => {
    if (nuclearTimeoutRef.current) {
      clearTimeout(nuclearTimeoutRef.current);
    }
    
    nuclearTimeoutRef.current = setTimeout(() => {
      console.log("☢️ NUCLEAR TIMEOUT: Forcing suggestions mode");
      setMode('suggestions');
    }, 3000); // 3 seconds MAXIMUM
  };

  // NUCLEAR: Cleanup on unmount
  useEffect(() => {
    return () => {
      if (nuclearTimeoutRef.current) {
        clearTimeout(nuclearTimeoutRef.current);
      }
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort();
        } catch (e) {}
      }
    };
  }, []);

  const toggleListening = () => {
    if (isListening) {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      return;
    }
    
    setShowBottomSheet(true);
    setMode('idle');
  };
  
  const startListening = async () => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      toast({
        title: "Speech Recognition Not Supported",
        description: "Your browser doesn't support speech recognition.",
        variant: "destructive"
      });
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop());

      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
      
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.lang = 'en-US';
      recognitionRef.current.continuous = false;
      recognitionRef.current.interimResults = false;
      
      recognitionRef.current.onresult = (event: any) => {
        const transcript = event.results[0][0].transcript;
        console.log("☢️ Voice input:", transcript);
        
        setIsListening(false);
        setHasAskedQuestion(true);
        setMode('thinking');
        
        // Start nuclear timeout immediately
        startNuclearTimeout();
        
        // Send message
        onSendMessage(transcript);
      };
      
      recognitionRef.current.onend = () => setIsListening(false);
      recognitionRef.current.onstart = () => setIsListening(true);
      recognitionRef.current.onerror = () => setIsListening(false);
      
      recognitionRef.current.start();
    } catch (error) {
      toast({
        title: "Microphone Error",
        description: "Cannot access microphone.",
        variant: "destructive"
      });
    }
  };

  const handleSuggestionClick = async (suggestion: string) => {
    console.log("☢️ Suggestion clicked:", suggestion);
    setHasAskedQuestion(true);
    setMode('thinking');
    startNuclearTimeout();
    
    try {
      await onSendMessage(suggestion);
    } catch (error) {
      setMode('suggestions');
    }
  };

  const handleCloseBottomSheet = () => {
    if (nuclearTimeoutRef.current) {
      clearTimeout(nuclearTimeoutRef.current);
    }
    
    if (hasAskedQuestion) {
      toast({
        description: "This conversation is saved in My cellar",
        duration: 3000,
      });
    }
    setShowBottomSheet(false);
    setMode('idle');
  };

  // NUCLEAR: Simple mode display
  const getDisplayMode = () => {
    if (isListening) return 'listening';
    if (mode === 'thinking') return 'thinking';
    if (mode === 'suggestions') return 'suggestions';
    return 'ask';
  };

  const displayMode = getDisplayMode();

  return (
    <div className="flex items-center">
      {/* NUCLEAR DEBUG */}
      <div style={{
        position: 'fixed',
        top: '10px',
        left: '10px',
        backgroundColor: 'red',
        color: 'white',
        padding: '5px',
        fontSize: '10px',
        zIndex: 10000
      }}>
        NUCLEAR MODE: {displayMode}<br/>
        isProcessing: {isProcessing.toString()}<br/>
        mode: {mode}
      </div>
      
      <div
        style={{
          width: '40px',
          height: '40px',
          backgroundColor: 'rgba(255, 255, 255, 0.1)',
          borderRadius: '50%',
          display: 'flex',
          justifyContent: 'center',
          alignItems: 'center',
          cursor: 'pointer'
        }}
        onClick={toggleListening}
      >
        <svg 
          xmlns="http://www.w3.org/2000/svg" 
          width="24" 
          height="24" 
          viewBox="0 0 20 20"
          style={{ color: 'white' }}
        >
          <path fill="currentColor" d="M5.5 10a.5.5 0 0 0-1 0a5.5 5.5 0 0 0 5 5.478V17.5a.5.5 0 0 0 1 0v-2.022a5.5 5.5 0 0 0 5-5.478a.5.5 0 0 0-1 0a4.5 4.5 0 1 1-9 0m7.5 0a3 3 0 0 1-6 0V5a3 3 0 0 1 6 0z"/>
        </svg>
      </div>
      
      <VoiceBottomSheet 
        isOpen={showBottomSheet} 
        onClose={handleCloseBottomSheet}
        onMute={() => setMode('suggestions')}
        onAsk={startListening}
        isListening={displayMode === 'listening'}
        isResponding={false}
        isThinking={displayMode === 'thinking'}
        showSuggestions={displayMode === 'suggestions'}
        onSuggestionClick={handleSuggestionClick}
      />
    </div>
  );
};

export default VoiceAssistant;