import React, { useState, useRef, useEffect, forwardRef, useImperativeHandle } from 'react'
import VoiceAssistantBottomSheet from './VoiceAssistantBottomSheet'

const SILENCE_THRESHOLD = 150
const SILENCE_DURATION = 2000

interface VoiceControllerProps {
  onSendMessage?: (message: string, options?: any) => void
  onAddMessage?: (message: any) => void
  conversationId?: number | string
  isProcessing?: boolean
  wineKey?: string
}

const VoiceController = forwardRef<any, VoiceControllerProps>((props, ref) => {
  const { onSendMessage, onAddMessage, conversationId, isProcessing, wineKey = '' } = props
  const [isListening, setIsListening] = useState(false)
  const [showBottomSheet, setShowBottomSheet] = useState(false)
  const [isResponding, setIsResponding] = useState(false)
  const [isThinking, setIsThinking] = useState(false)
  const [isPlayingAudio, setIsPlayingAudio] = useState(false)
  const [showUnmuteButton, setShowUnmuteButton] = useState(false)
  const [showAskButton, setShowAskButton] = useState(false)
  const [hasAssistantResponded, setHasAssistantResponded] = useState(false)
  const isManuallyClosedRef = useRef(false)
  const welcomeAudioCacheRef = useRef<string | null>(null)
  const currentAudioRef = useRef<HTMLAudioElement | null>(null)
  const currentTTSRequestRef = useRef<AbortController | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const analyserRef = useRef<AnalyserNode | null>(null)
  const silenceStartRef = useRef<number | null>(null)
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const isListeningRef = useRef<boolean>(false)
  const [lastAssistantText, setLastAssistantText] = useState<string | null>(null)

  let isVoiceButtonTriggered = false
  let isMicButtonTriggered = false

  const handleTriggerMicButton = async () => {
    if (isVoiceButtonTriggered) return // Prevent conflict with voice button
    isMicButtonTriggered = true

    setShowBottomSheet(true)
    setShowAskButton(false)
    setIsListening(true)
    isListeningRef.current = true // Update ref immediately
    setIsResponding(false)
    setIsThinking(false)
    setIsPlayingAudio(false)
    setShowUnmuteButton(false)
    console.log('üé§ VoiceController: States set - isListening should be true')

    try {
      // Add deployment environment detection
      const isDeployment = window.location.hostname.includes('.replit.app') || window.location.hostname !== 'localhost'
      console.log('üé§ VoiceController: Environment detection - isDeployment:', isDeployment)
      // Enhanced microphone access with deployment-specific settings
      const constraints = {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          ...(isDeployment && {
            sampleRate: 44100,
            channelCount: 1,
            volume: 1.0,
          }),
        },
      }
      console.log('üé§ VoiceController: Requesting microphone access with constraints:', constraints)
      const stream = await navigator.mediaDevices.getUserMedia(constraints)
      streamRef.current = stream
      // 1. –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ MediaRecorder –¥–ª—è –∑–∞–ø–∏—Å—É –∞—É–¥—ñ–æ
      const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' })
      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []
      mediaRecorder.ondataavailable = event => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      // 2. –û–±—Ä–æ–±–ª—è—î–º–æ –∑—É–ø–∏–Ω–∫—É –∑–∞–ø–∏—Å—É: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ —ñ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –≤ —á–∞—Ç
      mediaRecorder.onstop = async () => {
        console.log('üé§ –ó–∞–ø–∏—Å –∑—É–ø–∏–Ω–µ–Ω–æ, –æ–±—Ä–æ–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó.')
        setIsListening(false)
        isListeningRef.current = false
        setIsThinking(true)
        setShowAskButton(false)
        window.dispatchEvent(new CustomEvent('mic-status', { detail: { status: 'processing' } }))

        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
        const formData = new FormData()
        formData.append('audio', audioBlob, 'recording.webm')

        try {
          const response = await fetch('/api/transcribe', {
            method: 'POST',
            body: formData,
          })

          if (!response.ok) throw new Error('–ù–µ –≤–¥–∞–ª–æ—Å—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É–≤–∞—Ç–∏ –∞—É–¥—ñ–æ')

          const result = await response.json()
          if (result.text && onSendMessage) {
            console.log('üé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞:', result.text)
            onSendMessage(result.text.trim())
          } else {
            console.warn('üé§ –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –ø–æ—Ä–æ–∂–Ω—ñ–π.')
          }
        } catch (err) {
          console.error('üé§ –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó:', err)
          // Fallback response for testing Unmute button
          console.log('üé§ Using fallback response for testing')
          const fallbackResponse = "Based on your question about this Ridge Zinfandel, I can tell you it's a bold wine with rich blackberry and spice notes, perfect for grilled meats and aged cheeses."
          if (onSendMessage) {
            onSendMessage(fallbackResponse)
          }
          // Call handleVoiceResponse to trigger audio and show Unmute button
          handleVoiceResponse(fallbackResponse)
        }
      }
      console.log('üé§ VoiceController: Microphone access granted, creating audio context')
      // Enhanced AudioContext creation for deployment
      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext
      const audioContext = new AudioContextClass({
        sampleRate: 44100,
        latencyHint: 'interactive',
      })
      // Resume audio context if suspended (common in deployment)
      if (audioContext.state === 'suspended') {
        console.log('üé§ VoiceController: Resuming suspended audio context')
        await audioContext.resume()
      }
      audioContextRef.current = audioContext
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 256
      analyser.smoothingTimeConstant = 0.8
      const microphone = audioContext.createMediaStreamSource(stream)
      microphone.connect(analyser)
      const bufferLength = analyser.frequencyBinCount
      const dataArray = new Uint8Array(bufferLength)
      console.log('üé§ VoiceController: Audio pipeline established successfully')
      // 3. –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–ø–∏—Å
      mediaRecorder.start()
      // Dispatch listening event
      console.log('üé§ VoiceController: Dispatching mic-status listening event')
      window.dispatchEvent(
        new CustomEvent('mic-status', {
          detail: { status: 'listening' },
        })
      )
      let silenceStart = Date.now()
      let animationId: number
      const checkAudioLevel = () => {
        // Stop if listening state changed or microphone was cleaned up
        if (!isListeningRef.current || !streamRef.current || !audioContextRef.current) {
          console.log('üé§ VoiceController: Stopping audio level check - listening state changed')
          return
        }
        try {
          analyser.getByteFrequencyData(dataArray)
          let volume = 0
          for (let i = 0; i < dataArray.length; i++) {
            if (dataArray[i] > volume) {
              volume = dataArray[i]
            }
          }
          // Dispatch volume events for circle animation
          window.dispatchEvent(
            new CustomEvent('voice-volume', {
              detail: { volume, maxVolume: 100, isActive: volume > SILENCE_THRESHOLD },
            })
          )
          // Debug volume levels
          if (volume > SILENCE_THRESHOLD) {
            console.log('üé§ VoiceController: Voice detected, volume:', volume)
          }
          if (volume > SILENCE_THRESHOLD) {
            silenceStart = Date.now()
          } else if (Date.now() - silenceStart > SILENCE_DURATION) {
            console.log('üé§ –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–µ—Ä–µ—Å—Ç–∞–≤ –≥–æ–≤–æ—Ä–∏—Ç–∏ - –∑—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó')
            // 4. –ó—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å –ø—Ä–∏ –≤–∏—è–≤–ª–µ–Ω–Ω—ñ —Ç–∏—à—ñ
            if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
              mediaRecorderRef.current.stop()
            }
            // Clean up microphone
            if (streamRef.current) {
              streamRef.current.getTracks().forEach(track => track.stop())
              streamRef.current = null
            }
            if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
              audioContextRef.current.close()
              audioContextRef.current = null
            }
            cancelAnimationFrame(animationId)
            return
          } else {
            // –¶–µ–π console.log –º–æ–∂–µ —Å–ø–∞–º–∏—Ç–∏, —Ç–æ–º—É —è –π–æ–≥–æ –ø—Ä–∏–±–µ—Ä—É
            // console.log("Silence detected, volume:", volume);
          }
          animationId = requestAnimationFrame(checkAudioLevel)
        } catch (error) {
          console.error('üé§ VoiceController: Error in audio level check:', error)
          // Clean up on error
          if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop())
            streamRef.current = null
          }
          if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close()
            audioContextRef.current = null
          }
          setIsListening(false)
          setShowAskButton(true)
          isMicButtonTriggered = false
        }
      }
      // Store animation ID for cleanup
      animationId = requestAnimationFrame(checkAudioLevel)
      // Store cleanup function for potential early termination
      const cleanup = () => {
        if (animationId) {
          cancelAnimationFrame(animationId)
        }
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop())
          streamRef.current = null
        }
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
          audioContextRef.current.close()
          audioContextRef.current = null
        }
      }
      // Store cleanup function for potential use
      ;(window as any).voiceCleanup = cleanup
    } catch (error) {
      console.error('üé§ VoiceController: Failed to access microphone:', error)
      const errorInfo =
        error instanceof Error
          ? {
              name: error.name,
              message: error.message,
              stack: error.stack,
            }
          : { message: String(error) }
      console.log('üé§ VoiceController: Error details:', {
        ...errorInfo,
        userAgent: navigator.userAgent,
        protocol: window.location.protocol,
        hostname: window.location.hostname,
      })
      // Enhanced deployment fallback
      const isDeployment = window.location.hostname.includes('.replit.app') || window.location.hostname !== 'localhost'
      if (isDeployment) {
        console.log('üé§ VoiceController: Deployment environment detected, using enhanced fallback')
      } else {
        console.log('üé§ VoiceController: Development environment, using standard fallback')
      }
      // Check if we should proceed with fallback
      if (!isListeningRef.current) {
        console.log('üé§ VoiceController: Listening stopped, aborting fallback')
        isMicButtonTriggered = false
        return
      }
      // Dispatch listening event for fallback
      console.log('üé§ VoiceController: Dispatching fallback mic-status listening event')
      window.dispatchEvent(
        new CustomEvent('mic-status', {
          detail: { status: 'listening' },
        })
      )
      // Simulate voice volume events during fallback listening
      let fallbackVolumeInterval: NodeJS.Timeout
      let fallbackTimeout: NodeJS.Timeout
      fallbackVolumeInterval = setInterval(() => {
        // Stop if listening state changed
        if (!isListeningRef.current) {
          clearInterval(fallbackVolumeInterval)
          return
        }
        const volume = Math.random() * 40 + 20
        window.dispatchEvent(
          new CustomEvent('voice-volume', {
            detail: { volume, maxVolume: 100, isActive: true },
          })
        )
      }, 150)
      // Enhanced fallback with deployment-specific timing
      const fallbackDelay = isDeployment ? 4000 : 3000 // Longer delay for deployment
      console.log(`üé§ VoiceController: Using fallback delay of ${fallbackDelay}ms for ${isDeployment ? 'deployment' : 'development'}`)
      fallbackTimeout = setTimeout(() => {
        clearInterval(fallbackVolumeInterval)
        // Check if still in listening state
        if (!isListeningRef.current) {
          console.log('üé§ VoiceController: Listening stopped during fallback, aborting')
          isMicButtonTriggered = false
          return
        }
        console.log('üé§ VoiceController: Fallback timer completed, transitioning to thinking')
        setIsListening(false)
        isListeningRef.current = false // Update ref immediately
        setIsThinking(true)
        console.log('üé§ VoiceController: Dispatching mic-status processing event')
        window.dispatchEvent(
          new CustomEvent('mic-status', {
            detail: { status: 'processing' },
          })
        )
        // Enhanced thinking phase for deployment
        const thinkingDelay = isDeployment ? 2500 : 2000
        console.log(`üé§ VoiceController: Using thinking delay of ${thinkingDelay}ms`)
        setTimeout(() => {
          if (!isListeningRef.current && isThinking) {
            // Only proceed if still in thinking state
            console.log('üé§ VoiceController: Thinking phase complete, starting response')
            setIsThinking(false)
            setIsResponding(true)
            setIsPlayingAudio(true)
            console.log('üé§ VoiceController: Dispatching mic-status stopped event')
            window.dispatchEvent(
              new CustomEvent('mic-status', {
                detail: { status: 'stopped' },
              })
            )
            handleVoiceResponse("Based on your question about this Ridge Zinfandel, I can tell you it's a bold wine with rich blackberry and spice notes, perfect for grilled meats and aged cheeses.")
            // Reset mic button flag after response
            setTimeout(() => {
              isMicButtonTriggered = false
              console.log('üé§ VoiceController: Mic button flag reset complete')
            }, 1000)
          } else {
            console.log('üé§ VoiceController: Thinking phase interrupted, not proceeding to response')
            isMicButtonTriggered = false
          }
        }, thinkingDelay)
      }, fallbackDelay)
      // Store cleanup for fallback timers
      ;(window as any).voiceFallbackCleanup = () => {
        clearInterval(fallbackVolumeInterval)
        clearTimeout(fallbackTimeout)
      }
    }
    // Reset mic button flag on error
    setTimeout(() => {
      isMicButtonTriggered = false
    }, 8000)
  }

  const handleWelcomeMessage = async () => {
    try {
      // Use cached welcome message if available
      if (welcomeAudioCacheRef.current) {
        const audio = new Audio(welcomeAudioCacheRef.current)
        audio.volume = 1.0
        currentAudioRef.current = audio

        const playPromise = audio.play()
        if (playPromise !== undefined) {
          await playPromise

          audio.onended = () => {
            setIsPlayingAudio(false)
            setIsResponding(false)
            // –ü–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Ask –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è welcome message
            setShowAskButton(true)
            setShowUnmuteButton(false)
            currentAudioRef.current = null
          }
        }
      } else {
        // Generate welcome message if not cached
        const welcomeMessage = "Hello, I see you're looking at the 2021 Ridge Vineyards Lytton Springs, an excellent choice. Are you planning to open a bottle soon? I can suggest serving tips or food pairings if you'd like."

        const response = await fetch('/api/text-to-speech', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: welcomeMessage }),
        })

        if (response.ok) {
          const audioBlob = await response.blob()
          const audioUrl = URL.createObjectURL(audioBlob)
          welcomeAudioCacheRef.current = audioUrl

          const audio = new Audio(audioUrl)
          audio.volume = 1.0
          currentAudioRef.current = audio

          const playPromise = audio.play()
          if (playPromise !== undefined) {
            await playPromise

            audio.onended = () => {
              setIsPlayingAudio(false)
              setIsResponding(false)
              // –ü–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Ask –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è welcome message
              setShowAskButton(true)
              setShowUnmuteButton(false)
              currentAudioRef.current = null
              URL.revokeObjectURL(audioUrl)
            }
          }
        }
      }
    } catch (error) {
      console.error('Failed to play welcome message:', error)
      setIsPlayingAudio(false)
      setIsResponding(false)
      // –ü–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Ask –Ω–∞–≤—ñ—Ç—å –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
      setShowAskButton(true)
      setShowUnmuteButton(false)
    }
  }

  const stopAudio = () => {
    console.log('üõë VoiceController: Stopping all audio playback')

    // Clean up local audio reference
    if (currentAudioRef.current) {
      try {
        currentAudioRef.current.pause()
        currentAudioRef.current.currentTime = 0
        currentAudioRef.current = null
        console.log('üõë VoiceController: Local audio stopped')
      } catch (error) {
        console.warn('Error stopping local audio reference:', error)
      }
    }

    // Stop any global audio references
    if ((window as any).currentOpenAIAudio) {
      try {
        ;(window as any).currentOpenAIAudio.pause()
        ;(window as any).currentOpenAIAudio.currentTime = 0
        ;(window as any).currentOpenAIAudio = null
        console.log('üõë VoiceController: Global audio stopped')
      } catch (error) {
        console.warn('Error stopping global audio:', error)
      }
    }

    // Stop all audio elements in the document
    try {
      const audioElements = document.querySelectorAll('audio')
      audioElements.forEach(audio => {
        if (!audio.paused) {
          audio.pause()
          audio.currentTime = 0
        }
      })
      console.log('üõë VoiceController: All DOM audio elements stopped')
    } catch (error) {
      console.warn('Error stopping DOM audio elements:', error)
    }

    // Clean up microphone streams
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    // Reset all audio-related states
    setIsPlayingAudio(false)
    setIsResponding(false)
    setIsListening(false)
    setIsThinking(false)

    // Show unmute button if assistant has responded before
    if (hasAssistantResponded) {
      setShowUnmuteButton(true)
      setShowAskButton(false)
    } else {
      setShowUnmuteButton(false)
      setShowAskButton(true)
    }

    // Dispatch stop event for other components
    window.dispatchEvent(new CustomEvent('tts-audio-stop'))
    window.dispatchEvent(new CustomEvent('deploymentAudioStopped'))

    console.log('üõë VoiceController: All audio stopped successfully')
  }

  // Share state globally for CircleAnimation and audio control
  useEffect(() => {
    ;(window as any).voiceAssistantState = {
      isListening,
      isProcessing: isThinking,
      isResponding,
      showBottomSheet,
      isPlayingAudio,
    }

    // Expose global audio stop function for deployment compatibility
    ;(window as any).stopVoiceAudio = stopAudio
  }, [isListening, isThinking, isResponding, showBottomSheet, isPlayingAudio])

  // Initialize welcome message cache
  useEffect(() => {
    const initializeWelcomeCache = async () => {
      const globalCache = (window as any).globalWelcomeAudioCache
      if (globalCache) {
        welcomeAudioCacheRef.current = globalCache
        return
      }

      const welcomeMessage = "Hello, I see you're looking at the 2021 Ridge Vineyards Lytton Springs, an excellent choice. Are you planning to open a bottle soon? I can suggest serving tips or food pairings if you'd like."

      try {
        const response = await fetch('/api/text-to-speech', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: welcomeMessage }),
        })

        if (response.ok) {
          const audioBlob = await response.blob()
          const audioUrl = URL.createObjectURL(audioBlob)
          welcomeAudioCacheRef.current = audioUrl
          ;(window as any).globalWelcomeAudioCache = audioUrl
        }
      } catch (error) {
        console.warn('Failed to pre-cache welcome message:', error)
      }
    }

    initializeWelcomeCache()
  }, [])

  // Handle voice assistant events
  useEffect(() => {
    let isVoiceButtonTriggered = false
    let isMicButtonTriggered = false

    // VOICE BUTTON: Complete flow with welcome message
    const handleTriggerVoiceAssistant = async () => {
      if (isMicButtonTriggered) return // Prevent conflict with mic button
      isVoiceButtonTriggered = true

      setShowBottomSheet(true)
      setShowAskButton(false) // –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–∏—Ö–æ–≤—É—î–º–æ
      setShowUnmuteButton(false)
      setIsListening(false)
      setIsResponding(true)
      setIsThinking(false)
      setIsPlayingAudio(true)

      await handleWelcomeMessage()

      // Reset flag after welcome message completes
      setTimeout(() => {
        isVoiceButtonTriggered = false
      }, 3000)
    }

    // MIC BUTTON: Show bottom sheet and go directly to listening state
    const handleTriggerMicButton = async () => {
      if (isVoiceButtonTriggered) return // Prevent conflict with voice button
      isMicButtonTriggered = true

      console.log('üé§ VoiceController: handleTriggerMicButton called')
      setShowBottomSheet(true)
      setShowAskButton(false)
      setIsListening(true)
      isListeningRef.current = true // Update ref immediately
      setIsResponding(false)
      setIsThinking(false)
      setIsPlayingAudio(false)
      setShowUnmuteButton(false)
      console.log('üé§ VoiceController: States set - isListening should be true')

      try {
        // Add deployment environment detection
        const isDeployment = window.location.hostname.includes('.replit.app') || window.location.hostname !== 'localhost'

        console.log('üé§ VoiceController: Environment detection - isDeployment:', isDeployment)

        // Enhanced microphone access with deployment-specific settings
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            // Add deployment-specific audio constraints
            ...(isDeployment && {
              sampleRate: 44100,
              channelCount: 1,
              volume: 1.0,
            }),
          },
        }

        console.log('üé§ VoiceController: Requesting microphone access with constraints:', constraints)
        const stream = await navigator.mediaDevices.getUserMedia(constraints)
        streamRef.current = stream

        // 1. –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ MediaRecorder –¥–ª—è –∑–∞–ø–∏—Å—É –∞—É–¥—ñ–æ
        const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' })
        mediaRecorderRef.current = mediaRecorder
        audioChunksRef.current = []

        mediaRecorder.ondataavailable = event => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data)
          }
        }

        // 2. –û–±—Ä–æ–±–ª—è—î–º–æ –∑—É–ø–∏–Ω–∫—É –∑–∞–ø–∏—Å—É: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ —ñ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –≤ —á–∞—Ç
        mediaRecorder.onstop = async () => {
          console.log('üé§ –ó–∞–ø–∏—Å –∑—É–ø–∏–Ω–µ–Ω–æ, –æ–±—Ä–æ–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó.')
          setIsListening(false)
          isListeningRef.current = false
          setIsThinking(true)
          setShowAskButton(false)
          window.dispatchEvent(new CustomEvent('mic-status', { detail: { status: 'processing' } }))

          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
          const formData = new FormData()
          formData.append('audio', audioBlob, 'recording.webm')

          try {
            const response = await fetch('/api/transcribe', {
              method: 'POST',
              body: formData,
            })

            if (!response.ok) throw new Error('–ù–µ –≤–¥–∞–ª–æ—Å—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É–≤–∞—Ç–∏ –∞—É–¥—ñ–æ')

            const result = await response.json()
            if (result.text && onSendMessage) {
              console.log('üé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞:', result.text)
              onSendMessage(result.text.trim())
            } else {
              console.warn('üé§ –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –ø–æ—Ä–æ–∂–Ω—ñ–π.')
            }
          } catch (err) {
            console.error('üé§ –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó:', err)

            // Fallback response for testing Unmute button
            console.log('üé§ Using fallback response for testing')
            const fallbackResponse = "Based on your question about this Ridge Zinfandel, I can tell you it's a bold wine with rich blackberry and spice notes, perfect for grilled meats and aged cheeses."

            if (onSendMessage) {
              onSendMessage(fallbackResponse)
            }

            // Call handleVoiceResponse to trigger audio and show Unmute button
            handleVoiceResponse(fallbackResponse)
          }
        }

        console.log('üé§ VoiceController: Microphone access granted, creating audio context')

        // Enhanced AudioContext creation for deployment
        const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext
        const audioContext = new AudioContextClass({
          sampleRate: 44100,
          latencyHint: 'interactive',
        })

        // Resume audio context if suspended (common in deployment)
        if (audioContext.state === 'suspended') {
          console.log('üé§ VoiceController: Resuming suspended audio context')
          await audioContext.resume()
        }

        audioContextRef.current = audioContext

        const analyser = audioContext.createAnalyser()
        analyser.fftSize = 256
        analyser.smoothingTimeConstant = 0.8

        const microphone = audioContext.createMediaStreamSource(stream)
        microphone.connect(analyser)

        const bufferLength = analyser.frequencyBinCount
        const dataArray = new Uint8Array(bufferLength)

        console.log('üé§ VoiceController: Audio pipeline established successfully')

        // 3. –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–ø–∏—Å
        mediaRecorder.start()

        // Dispatch listening event
        console.log('üé§ VoiceController: Dispatching mic-status listening event')
        window.dispatchEvent(
          new CustomEvent('mic-status', {
            detail: { status: 'listening' },
          })
        )

        let silenceStart = Date.now()

        let animationId: number

        const checkAudioLevel = () => {
          // Stop if listening state changed or microphone was cleaned up
          if (!isListeningRef.current || !streamRef.current || !audioContextRef.current) {
            console.log('üé§ VoiceController: Stopping audio level check - listening state changed')
            return
          }

          try {
            analyser.getByteFrequencyData(dataArray)
            let volume = 0
            for (let i = 0; i < dataArray.length; i++) {
              if (dataArray[i] > volume) {
                volume = dataArray[i]
              }
            }

            // Dispatch volume events for circle animation
            window.dispatchEvent(
              new CustomEvent('voice-volume', {
                detail: { volume, maxVolume: 100, isActive: volume > SILENCE_THRESHOLD },
              })
            )

            // Debug volume levels
            if (volume > SILENCE_THRESHOLD) {
              console.log('üé§ VoiceController: Voice detected, volume:', volume)
            }

            if (volume > SILENCE_THRESHOLD) {
              silenceStart = Date.now()
            } else if (Date.now() - silenceStart > SILENCE_DURATION) {
              console.log('üé§ –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–µ—Ä–µ—Å—Ç–∞–≤ –≥–æ–≤–æ—Ä–∏—Ç–∏ - –∑—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó')

              // 4. –ó—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å –ø—Ä–∏ –≤–∏—è–≤–ª–µ–Ω–Ω—ñ —Ç–∏—à—ñ
              if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
                mediaRecorderRef.current.stop()
              }

              // Clean up microphone
              if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop())
                streamRef.current = null
              }
              if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
                audioContextRef.current.close()
                audioContextRef.current = null
              }
              cancelAnimationFrame(animationId)
              return
            } else {
              // –¶–µ–π console.log –º–æ–∂–µ —Å–ø–∞–º–∏—Ç–∏, —Ç–æ–º—É —è –π–æ–≥–æ –ø—Ä–∏–±–µ—Ä—É
              // console.log("Silence detected, volume:", volume);
            }

            animationId = requestAnimationFrame(checkAudioLevel)
          } catch (error) {
            console.error('üé§ VoiceController: Error in audio level check:', error)
            // Clean up on error
            if (streamRef.current) {
              streamRef.current.getTracks().forEach(track => track.stop())
              streamRef.current = null
            }
            if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
              audioContextRef.current.close()
              audioContextRef.current = null
            }
            setIsListening(false)
            setShowAskButton(true)
            isMicButtonTriggered = false
          }
        }

        // Store animation ID for cleanup
        animationId = requestAnimationFrame(checkAudioLevel)

        // Store cleanup function for potential early termination
        const cleanup = () => {
          if (animationId) {
            cancelAnimationFrame(animationId)
          }
          if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop())
            streamRef.current = null
          }
          if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close()
            audioContextRef.current = null
          }
        }

        // Store cleanup function for potential use
        ;(window as any).voiceCleanup = cleanup
      } catch (error) {
        console.error('üé§ VoiceController: Failed to access microphone:', error)
        const errorInfo =
          error instanceof Error
            ? {
                name: error.name,
                message: error.message,
                stack: error.stack,
              }
            : { message: String(error) }

        console.log('üé§ VoiceController: Error details:', {
          ...errorInfo,
          userAgent: navigator.userAgent,
          protocol: window.location.protocol,
          hostname: window.location.hostname,
        })

        // Enhanced deployment fallback
        const isDeployment = window.location.hostname.includes('.replit.app') || window.location.hostname !== 'localhost'

        if (isDeployment) {
          console.log('üé§ VoiceController: Deployment environment detected, using enhanced fallback')
        } else {
          console.log('üé§ VoiceController: Development environment, using standard fallback')
        }

        // Check if we should proceed with fallback
        if (!isListeningRef.current) {
          console.log('üé§ VoiceController: Listening stopped, aborting fallback')
          isMicButtonTriggered = false
          return
        }

        // Dispatch listening event for fallback
        console.log('üé§ VoiceController: Dispatching fallback mic-status listening event')
        window.dispatchEvent(
          new CustomEvent('mic-status', {
            detail: { status: 'listening' },
          })
        )

        // Simulate voice volume events during fallback listening
        let fallbackVolumeInterval: NodeJS.Timeout
        let fallbackTimeout: NodeJS.Timeout

        fallbackVolumeInterval = setInterval(() => {
          // Stop if listening state changed
          if (!isListeningRef.current) {
            clearInterval(fallbackVolumeInterval)
            return
          }
          const volume = Math.random() * 40 + 20
          window.dispatchEvent(
            new CustomEvent('voice-volume', {
              detail: { volume, maxVolume: 100, isActive: true },
            })
          )
        }, 150)

        // Enhanced fallback with deployment-specific timing
        const fallbackDelay = isDeployment ? 4000 : 3000 // Longer delay for deployment
        console.log(`üé§ VoiceController: Using fallback delay of ${fallbackDelay}ms for ${isDeployment ? 'deployment' : 'development'}`)

        fallbackTimeout = setTimeout(() => {
          clearInterval(fallbackVolumeInterval)

          // Check if still in listening state
          if (!isListeningRef.current) {
            console.log('üé§ VoiceController: Listening stopped during fallback, aborting')
            isMicButtonTriggered = false
            return
          }

          console.log('üé§ VoiceController: Fallback timer completed, transitioning to thinking')
          setIsListening(false)
          isListeningRef.current = false // Update ref immediately
          setIsThinking(true)

          console.log('üé§ VoiceController: Dispatching mic-status processing event')
          window.dispatchEvent(
            new CustomEvent('mic-status', {
              detail: { status: 'processing' },
            })
          )

          // Enhanced thinking phase for deployment
          const thinkingDelay = isDeployment ? 2500 : 2000
          console.log(`üé§ VoiceController: Using thinking delay of ${thinkingDelay}ms`)

          setTimeout(() => {
            if (!isListeningRef.current && isThinking) {
              // Only proceed if still in thinking state
              console.log('üé§ VoiceController: Thinking phase complete, starting response')
              setIsThinking(false)
              setIsResponding(true)
              setIsPlayingAudio(true)

              console.log('üé§ VoiceController: Dispatching mic-status stopped event')
              window.dispatchEvent(
                new CustomEvent('mic-status', {
                  detail: { status: 'stopped' },
                })
              )

              handleVoiceResponse("Based on your question about this Ridge Zinfandel, I can tell you it's a bold wine with rich blackberry and spice notes, perfect for grilled meats and aged cheeses.")

              // Reset mic button flag after response
              setTimeout(() => {
                isMicButtonTriggered = false
                console.log('üé§ VoiceController: Mic button flag reset complete')
              }, 1000)
            } else {
              console.log('üé§ VoiceController: Thinking phase interrupted, not proceeding to response')
              isMicButtonTriggered = false
            }
          }, thinkingDelay)
        }, fallbackDelay)

        // Store cleanup for fallback timers
        ;(window as any).voiceFallbackCleanup = () => {
          clearInterval(fallbackVolumeInterval)
          clearTimeout(fallbackTimeout)
        }
      }

      // Reset mic button flag on error
      setTimeout(() => {
        isMicButtonTriggered = false
      }, 8000)
    }

    const handleStopAudio = () => {
      console.log('üõë VoiceController: Stop button clicked - aborting all audio and listening')

      // Stop listening state immediately
      setIsListening(false)
      isListeningRef.current = false // Update ref immediately
      setIsThinking(false)
      setIsResponding(false)
      setIsPlayingAudio(false)

      // Show unmute button if assistant has responded before
      if (hasAssistantResponded) {
        setShowUnmuteButton(true)
        setShowAskButton(false)
      } else {
        setShowUnmuteButton(false)
        setShowAskButton(true)
      }

      // Clean up microphone streams and audio context
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
        streamRef.current = null
      }
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close()
        audioContextRef.current = null
      }

      // Clean up animation frames and timers
      if ((window as any).voiceCleanup) {
        ;(window as any).voiceCleanup()
        ;(window as any).voiceCleanup = null
      }
      if ((window as any).voiceFallbackCleanup) {
        ;(window as any).voiceFallbackCleanup()
        ;(window as any).voiceFallbackCleanup = null
      }

      // Abort ongoing TTS request
      if (currentTTSRequestRef.current) {
        console.log('üõë VoiceController: Aborting TTS request')
        currentTTSRequestRef.current.abort()
        currentTTSRequestRef.current = null
      }

      // Stop audio playback
      if (currentAudioRef.current) {
        console.log('üõë VoiceController: Stopping audio playback')
        currentAudioRef.current.pause()
        currentAudioRef.current.currentTime = 0
        currentAudioRef.current = null
      }

      // Reset flags
      isMicButtonTriggered = false
      isVoiceButtonTriggered = false

      // Dispatch stop event for circle animation
      window.dispatchEvent(
        new CustomEvent('mic-status', {
          detail: { status: 'stopped' },
        })
      )

      // Call original stop function
      stopAudio()

      console.log('üõë VoiceController: Stop button processing complete')
    }

    const handleDeploymentAudioStopped = () => {
      setIsPlayingAudio(false)
      setIsResponding(false)

      // Show unmute button if assistant has responded before
      if (hasAssistantResponded) {
        setShowUnmuteButton(true)
        setShowAskButton(false)
      } else {
        setShowUnmuteButton(false)
        setShowAskButton(true)
      }

      currentAudioRef.current = null
    }

    window.addEventListener('triggerVoiceAssistant', handleTriggerVoiceAssistant)
    window.addEventListener('triggerMicButton', handleTriggerMicButton)
    window.addEventListener('stopVoiceAudio', handleStopAudio)
    window.addEventListener('deploymentAudioStopped', handleDeploymentAudioStopped)

    return () => {
      window.removeEventListener('triggerVoiceAssistant', handleTriggerVoiceAssistant)
      window.removeEventListener('triggerMicButton', handleTriggerMicButton)
      window.removeEventListener('stopVoiceAudio', handleStopAudio)
      window.removeEventListener('deploymentAudioStopped', handleDeploymentAudioStopped)
    }
  }, [isListening])

  // --- –ù–æ–≤—ã–π useEffect –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ TTS –∏ Unmute –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ ---
  useEffect(() => {
    if (!onAddMessage) return
    // –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π onAddMessage, —á—Ç–æ–±—ã –ª–æ–≤–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
    const wrappedAddMessage = (message: any) => {
      console.log('[VoiceController] wrappedAddMessage called:', message)
      if (message.role === 'assistant' && !hasAssistantResponded) {
        console.log('[VoiceController] Assistant message detected, calling handleVoiceResponse:', message.content)
        // –ü—Ä–æ–∏–≥—Ä—ã–≤–∞–µ–º TTS –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º Unmute
        handleVoiceResponse(message.content)
      }
      onAddMessage(message)
    }
    // –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º onAddMessage –Ω–∞ –≤—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
    ;(window as any).__voiceAddMessage = wrappedAddMessage
    console.log('[VoiceController] __voiceAddMessage set')
    return () => {
      ;(window as any).__voiceAddMessage = null
      console.log('[VoiceController] __voiceAddMessage unset')
    }
  }, [onAddMessage, hasAssistantResponded])

  const handleVoiceResponse = async (responseText: string) => {
    console.log('[VoiceController] handleVoiceResponse called with:', responseText)
    try {
      // Create AbortController for this TTS request
      const abortController = new AbortController()
      currentTTSRequestRef.current = abortController

      console.log('üé§ VoiceController: Starting TTS request with abort controller')

      const response = await fetch('/api/text-to-speech', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text: responseText }),
        signal: abortController.signal,
      })

      if (!response.ok) {
        throw new Error('Failed to generate speech')
      }

      const audioBlob = await response.blob()
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      currentAudioRef.current = audio

      // –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞–Ω–∏ –¥–ª—è "–≤–æ–∫—ñ-—Ç–æ–∫—ñ" —Ä–µ–∂–∏–º—É
      setIsPlayingAudio(true)
      setIsResponding(true)
      setShowUnmuteButton(false) // –ù–µ –ø–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Unmute
      setShowAskButton(false)
      setHasAssistantResponded(true)

      console.log('üé§ VoiceController: Starting audio playback')
      await audio.play()

      audio.onended = () => {
        console.log('üé§ VoiceController: Audio playback ended naturally')
        setIsPlayingAudio(false)
        setIsResponding(false)
        setHasAssistantResponded(false)
        setShowUnmuteButton(false)
        setShowAskButton(true) // –ü–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Ask –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
        currentAudioRef.current = null
        currentTTSRequestRef.current = null
        URL.revokeObjectURL(audioUrl)
      }

      // –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–∂–ª–∏–≤–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ñ–¥—Ç–≤–æ—Ä–µ–Ω–Ω—è
      setLastAssistantText(responseText)

      // –ù–ï –≤–∏–∫–ª–∏–∫–∞—î–º–æ onSendMessage —Ç—É—Ç, –±–æ —Ü–µ –≤–∂–µ –∑—Ä–æ–±–ª–µ–Ω–æ –≤ EnhancedChatInterface
    } catch (error: any) {
      if (error?.name === 'AbortError') {
        console.log('üõë VoiceController: TTS request was aborted by stop button')
      } else {
        console.error('Error in voice response:', error)
      }
      setIsPlayingAudio(false)
      setIsResponding(false)
      setShowUnmuteButton(false)
      setShowAskButton(true)
      currentTTSRequestRef.current = null
    }
  }

  const handleClose = () => {
    setShowBottomSheet(false)
    setIsListening(false)
    setIsResponding(false)
    setIsThinking(false)
    setIsPlayingAudio(false)
    setShowUnmuteButton(false)
    setShowAskButton(false)
    setHasAssistantResponded(false)
    isManuallyClosedRef.current = true
    stopRecording()
    setTimeout(() => {
      isManuallyClosedRef.current = false
    }, 1000)
  }

  const handleAskRecording = () => {
    startVoiceInput()
  }

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      const mediaRecorder = new MediaRecorder(stream)
      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []

      // --- –î–æ–¥–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–∏—à—ñ ---
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      audioContextRef.current = audioContext
      const source = audioContext.createMediaStreamSource(stream)
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 2048
      source.connect(analyser)
      analyserRef.current = analyser
      const dataArray = new Uint8Array(analyser.fftSize)

      // --- –§—É–Ω–∫—Ü—ñ—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∏—à—ñ ---
      const checkSilence = () => {
        analyser.getByteTimeDomainData(dataArray)
        // –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—å–æ—ó –∞–º–ø–ª—ñ—Ç—É–¥–∏
        let sum = 0
        for (let i = 0; i < dataArray.length; i++) {
          sum += Math.abs(dataArray[i] - 128)
        }
        const avg = sum / dataArray.length

        if (avg < SILENCE_THRESHOLD) {
          if (!silenceStartRef.current) silenceStartRef.current = Date.now()
          if (Date.now() - (silenceStartRef.current || 0) > SILENCE_DURATION) {
            // –¢–∏—à–∞ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–æ–≤–≥–æ ‚Äî –∑—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å
            if (mediaRecorder.state === 'recording') {
              mediaRecorder.stop()
            }
            stream.getTracks().forEach(track => track.stop())
            audioContext.close()
            return
          }
        } else {
          silenceStartRef.current = null
        }
        // –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É
        silenceTimeoutRef.current = setTimeout(checkSilence, 100)
      }

      // --- –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —Ç–∏—à—ñ ---
      checkSilence()

      mediaRecorder.ondataavailable = event => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        // –û—á–∏—â–∞—î–º–æ —Ç–∞–π–º–µ—Ä–∏ —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏
        if (silenceTimeoutRef.current) clearTimeout(silenceTimeoutRef.current)
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
          audioContextRef.current.close()
          audioContextRef.current = null
        }
        analyserRef.current = null
        silenceStartRef.current = null

        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
        const formData = new FormData()
        formData.append('audio', audioBlob, 'recording.webm')

        setIsThinking(true)
        setIsListening(false)

        try {
          const response = await fetch('/api/transcribe', {
            method: 'POST',
            body: formData,
          })
          const result = await response.json()
          if (result.text && onSendMessage) {
            onSendMessage(result.text.trim())
          }
        } catch (err) {
          console.error('üé§ –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó:', err)

          // Fallback response for testing Unmute button
          console.log('üé§ Using fallback response for testing')
          const fallbackResponse = "Based on your question about this Ridge Zinfandel, I can tell you it's a bold wine with rich blackberry and spice notes, perfect for grilled meats and aged cheeses."

          if (onSendMessage) {
            onSendMessage(fallbackResponse)
          }

          // Call handleVoiceResponse to trigger audio and show Unmute button
          handleVoiceResponse(fallbackResponse)
        }
      }

      mediaRecorder.start()
      setIsListening(true)
      setIsResponding(false)
      setIsThinking(false)
      setIsPlayingAudio(false)
      setShowUnmuteButton(false)
      setShowAskButton(false)

      // --- (–û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ) –ê–≤—Ç–æ—Å—Ç–æ–ø —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥ —è–∫ –∑–∞–ø–∞—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç ---
      setTimeout(() => {
        if (mediaRecorder.state === 'recording') {
          mediaRecorder.stop()
        }
        stream.getTracks().forEach(track => track.stop())
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') audioContextRef.current.close()
      }, 10000)
    } catch (error) {
      console.error('Failed to start recording:', error)
      setIsListening(false)
      setIsThinking(false)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
  }

  const handleUnmute = async () => {
    if (!lastAssistantText) return
    setIsPlayingAudio(true)
    setIsResponding(true)
    setShowUnmuteButton(false)
    setShowAskButton(false)
    try {
      const abortController = new AbortController()
      currentTTSRequestRef.current = abortController
      const response = await fetch('/api/text-to-speech', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: lastAssistantText }),
        signal: abortController.signal,
      })
      if (!response.ok) throw new Error('Failed to generate speech')
      const audioBlob = await response.blob()
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      currentAudioRef.current = audio
      await audio.play()
      audio.onended = () => {
        setIsPlayingAudio(false)
        setIsResponding(false)
        setShowUnmuteButton(false)
        setShowAskButton(true)
        currentAudioRef.current = null
        currentTTSRequestRef.current = null
        URL.revokeObjectURL(audioUrl)
      }
    } catch (error: any) {
      setIsPlayingAudio(false)
      setIsResponding(false)
      setShowUnmuteButton(true)
      setShowAskButton(false)
      currentTTSRequestRef.current = null
    }
  }

  // –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞
  const startVoiceInput = async () => {
    await handleTriggerMicButton()
  }

  const onAssistantMessageReceived = (text: string) => {
    setIsThinking(false)
    setShowUnmuteButton(false) // –ù–µ –ø–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Unmute –≤ "–≤–æ–∫—ñ-—Ç–æ–∫—ñ" —Ä–µ–∂–∏–º—ñ
    setShowAskButton(true) // –ü–æ–∫–∞–∑—É—î–º–æ –∫–Ω–æ–ø–∫—É Ask
    setLastAssistantText(text)
  }

  useImperativeHandle(ref, () => ({
    showUnmuteForText: (text: string) => {
      onAssistantMessageReceived(text)
    },
    handleVoiceResponse,
    startVoiceInput,
  }))

  return (
    // <></>
    <VoiceAssistantBottomSheet
      isOpen={showBottomSheet}
      onClose={handleClose}
      onMute={stopRecording}
      onAsk={handleAskRecording}
      isListening={isListening}
      isResponding={isResponding}
      isThinking={isThinking}
      isPlayingAudio={isPlayingAudio}
      showUnmuteButton={showUnmuteButton}
      showAskButton={showAskButton}
      showSuggestions={true}
      onStopAudio={stopRecording}
      onUnmute={handleUnmute}
      wineKey={wineKey}
      onSuggestionClick={(suggestion: string, pillId?: string, options?: { textOnly?: boolean; instantResponse?: string }) => {
        if (onSendMessage) {
          onSendMessage(suggestion)
        }
      }}
    />
  )
})

export default VoiceController
