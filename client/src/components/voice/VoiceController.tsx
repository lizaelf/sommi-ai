import React, { useState, useRef, useEffect } from 'react';
import VoiceAssistantBottomSheet from '../bottom-sheet/VoiceAssistantBottomSheet';

const SILENCE_THRESHOLD = 150;
const SILENCE_DURATION = 2000;

interface VoiceControllerProps {
  onSendMessage?: (message: string, options?: any) => void;
  onAddMessage?: (message: any) => void;
  conversationId?: number;
  isProcessing?: boolean;
  wineKey?: string;
}

const VoiceController: React.FC<VoiceControllerProps> = ({
  onSendMessage,
  onAddMessage,
  conversationId,
  isProcessing,
  wineKey = '',
}) => {
  const [isListening, setIsListening] = useState(false);
  const [showBottomSheet, setShowBottomSheet] = useState(false);
  const [isResponding, setIsResponding] = useState(false);
  const [isThinking, setIsThinking] = useState(false);
  const [isPlayingAudio, setIsPlayingAudio] = useState(false);
  const [showUnmuteButton, setShowUnmuteButton] = useState(false);
  const [showAskButton, setShowAskButton] = useState(false);
  const isManuallyClosedRef = useRef(false);
  const welcomeAudioCacheRef = useRef<string | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const currentTTSRequestRef = useRef<AbortController | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceStartRef = useRef<number | null>(null);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const isListeningRef = useRef<boolean>(false);

  const handleWelcomeMessage = async () => {
    try {
      // Use cached welcome message if available
      if (welcomeAudioCacheRef.current) {
        const audio = new Audio(welcomeAudioCacheRef.current);
        audio.volume = 1.0;
        currentAudioRef.current = audio;
        
        const playPromise = audio.play();
        if (playPromise !== undefined) {
          await playPromise;
          
          audio.onended = () => {
            setIsPlayingAudio(false);
            setIsResponding(false);
            currentAudioRef.current = null;
          };
        }
      } else {
        // Generate welcome message if not cached
        const welcomeMessage = "Hello, I see you're looking at the 2021 Ridge Vineyards Lytton Springs, an excellent choice. Are you planning to open a bottle soon? I can suggest serving tips or food pairings if you'd like.";
        
        const response = await fetch('/api/text-to-speech', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: welcomeMessage }),
        });

        if (response.ok) {
          const audioBlob = await response.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          welcomeAudioCacheRef.current = audioUrl;
          
          const audio = new Audio(audioUrl);
          audio.volume = 1.0;
          currentAudioRef.current = audio;
          
          const playPromise = audio.play();
          if (playPromise !== undefined) {
            await playPromise;
            
            audio.onended = () => {
              setIsPlayingAudio(false);
              setIsResponding(false);
              currentAudioRef.current = null;
              URL.revokeObjectURL(audioUrl);
            };
          }
        }
      }
    } catch (error) {
      console.error("Failed to play welcome message:", error);
      setIsPlayingAudio(false);
      setIsResponding(false);
    }
  };

  const stopAudio = () => {
    console.log("üõë VoiceController: Stopping all audio playback");
    
    // Clean up local audio reference
    if (currentAudioRef.current) {
      try {
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current = null;
        console.log("üõë VoiceController: Local audio stopped");
      } catch (error) {
        console.warn("Error stopping local audio reference:", error);
      }
    }
    
    // Stop any global audio references
    if ((window as any).currentOpenAIAudio) {
      try {
        (window as any).currentOpenAIAudio.pause();
        (window as any).currentOpenAIAudio.currentTime = 0;
        (window as any).currentOpenAIAudio = null;
        console.log("üõë VoiceController: Global audio stopped");
      } catch (error) {
        console.warn("Error stopping global audio:", error);
      }
    }
    
    // Stop all audio elements in the document
    try {
      const audioElements = document.querySelectorAll('audio');
      audioElements.forEach((audio) => {
        if (!audio.paused) {
          audio.pause();
          audio.currentTime = 0;
        }
      });
      console.log("üõë VoiceController: All DOM audio elements stopped");
    } catch (error) {
      console.warn("Error stopping DOM audio elements:", error);
    }
    
    // Clean up microphone streams
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    // Reset all audio-related states
    setIsPlayingAudio(false);
    setIsResponding(false);
    setIsListening(false);
    setIsThinking(false);
    setShowUnmuteButton(false);
    setShowAskButton(true);
    
    // Dispatch stop event for other components
    window.dispatchEvent(new CustomEvent("tts-audio-stop"));
    window.dispatchEvent(new CustomEvent("deploymentAudioStopped"));
    
    console.log("üõë VoiceController: All audio stopped successfully");
  };

  // Share state globally for CircleAnimation and audio control
  useEffect(() => {
    (window as any).voiceAssistantState = {
      isListening,
      isProcessing: isThinking,
      isResponding,
      showBottomSheet,
      isPlayingAudio
    };
    
    // Expose global audio stop function for deployment compatibility
    (window as any).stopVoiceAudio = stopAudio;
  }, [isListening, isThinking, isResponding, showBottomSheet, isPlayingAudio]);

  // Initialize welcome message cache
  useEffect(() => {
    const initializeWelcomeCache = async () => {
      const globalCache = (window as any).globalWelcomeAudioCache;
      if (globalCache) {
        welcomeAudioCacheRef.current = globalCache;
        return;
      }

      const welcomeMessage = "Hello, I see you're looking at the 2021 Ridge Vineyards Lytton Springs, an excellent choice. Are you planning to open a bottle soon? I can suggest serving tips or food pairings if you'd like.";
      
      try {
        const response = await fetch('/api/text-to-speech', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: welcomeMessage }),
        });

        if (response.ok) {
          const audioBlob = await response.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          welcomeAudioCacheRef.current = audioUrl;
          (window as any).globalWelcomeAudioCache = audioUrl;
        }
      } catch (error) {
        console.warn("Failed to pre-cache welcome message:", error);
      }
    };

    initializeWelcomeCache();
  }, []);

  // Handle voice assistant events
  useEffect(() => {
    let isVoiceButtonTriggered = false;
    let isMicButtonTriggered = false;
    
    // VOICE BUTTON: Complete flow with welcome message
    const handleTriggerVoiceAssistant = async () => {
      if (isMicButtonTriggered) return; // Prevent conflict with mic button
      isVoiceButtonTriggered = true;
      
      setShowBottomSheet(true);
      setShowAskButton(false);
      setIsListening(false);
      setIsResponding(true);
      setIsThinking(false);
      setIsPlayingAudio(true);
      setShowUnmuteButton(false);
      
      await handleWelcomeMessage();
      
      // Reset flag after welcome message completes
      setTimeout(() => {
        isVoiceButtonTriggered = false;
      }, 3000);
    };

    // MIC BUTTON: Show bottom sheet and go directly to listening state
    const handleTriggerMicButton = async () => {
      if (isVoiceButtonTriggered) return; // Prevent conflict with voice button
      isMicButtonTriggered = true;
      
      console.log("üé§ VoiceController: handleTriggerMicButton called");
      setShowBottomSheet(true);
      setShowAskButton(false);
      setIsListening(true);
      isListeningRef.current = true; // Update ref immediately
      setIsResponding(false);
      setIsThinking(false);
      setIsPlayingAudio(false);
      setShowUnmuteButton(false);
      console.log("üé§ VoiceController: States set - isListening should be true");
      
      try {
        // Add deployment environment detection
        const isDeployment = window.location.hostname.includes('.replit.app') || 
                           window.location.hostname !== 'localhost';
        
        console.log("üé§ VoiceController: Environment detection - isDeployment:", isDeployment);
        
        // Enhanced microphone access with deployment-specific settings
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            // Add deployment-specific audio constraints
            ...(isDeployment && {
              sampleRate: 44100,
              channelCount: 1,
              volume: 1.0
            })
          }
        };
        
        console.log("üé§ VoiceController: Requesting microphone access with constraints:", constraints);
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        streamRef.current = stream;
        
        // 1. –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ MediaRecorder –¥–ª—è –∑–∞–ø–∏—Å—É –∞—É–¥—ñ–æ
        const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
        mediaRecorderRef.current = mediaRecorder;
        audioChunksRef.current = [];

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data);
          }
        };

        // 2. –û–±—Ä–æ–±–ª—è—î–º–æ –∑—É–ø–∏–Ω–∫—É –∑–∞–ø–∏—Å—É: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ —ñ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –≤ —á–∞—Ç
        mediaRecorder.onstop = async () => {
          console.log("üé§ –ó–∞–ø–∏—Å –∑—É–ø–∏–Ω–µ–Ω–æ, –æ–±—Ä–æ–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó.");
          setIsListening(false);
          isListeningRef.current = false;
          setIsThinking(true);
          window.dispatchEvent(new CustomEvent('mic-status', { detail: { status: 'processing' } }));

          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm;codecs=opus' });
          const formData = new FormData();
          formData.append('audio', audioBlob, 'recording.webm');

          try {
            const response = await fetch('/api/transcribe', {
              method: 'POST',
              body: formData,
            });

            if (!response.ok) throw new Error('–ù–µ –≤–¥–∞–ª–æ—Å—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É–≤–∞—Ç–∏ –∞—É–¥—ñ–æ');

            const result = await response.json();
            if (result.text && onSendMessage) {
              console.log("üé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞:", result.text);
              onSendMessage(result.text.trim());
            } else {
              console.warn("üé§ –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –ø–æ—Ä–æ–∂–Ω—ñ–π.");
            }
          } catch (err) {
            console.error("üé§ –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó:", err);
          } finally {
            setIsThinking(false);
            setShowAskButton(true);
          }
        };

        console.log("üé§ VoiceController: Microphone access granted, creating audio context");
        
        // Enhanced AudioContext creation for deployment
        const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
        const audioContext = new AudioContextClass({
          sampleRate: 44100,
          latencyHint: 'interactive'
        });
        
        // Resume audio context if suspended (common in deployment)
        if (audioContext.state === 'suspended') {
          console.log("üé§ VoiceController: Resuming suspended audio context");
          await audioContext.resume();
        }
        
        audioContextRef.current = audioContext;
        
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        analyser.smoothingTimeConstant = 0.8;
        
        const microphone = audioContext.createMediaStreamSource(stream);
        microphone.connect(analyser);
        
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        
        console.log("üé§ VoiceController: Audio pipeline established successfully");
        
        // 3. –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–ø–∏—Å
        mediaRecorder.start();
        
        // Dispatch listening event
        console.log("üé§ VoiceController: Dispatching mic-status listening event");
        window.dispatchEvent(new CustomEvent('mic-status', {
          detail: { status: 'listening' }
        }));
        
        let silenceStart = Date.now();
        
        let animationId: number;
        
        const checkAudioLevel = () => {
          // Stop if listening state changed or microphone was cleaned up
          if (!isListeningRef.current || !streamRef.current || !audioContextRef.current) {
            console.log("üé§ VoiceController: Stopping audio level check - listening state changed");
            return;
          }
          
          try {
            analyser.getByteFrequencyData(dataArray);
            let volume = 0;
            for (let i = 0; i < dataArray.length; i++) {
              if (dataArray[i] > volume) {
                volume = dataArray[i];
              }
            }
            
            // Dispatch volume events for circle animation
            window.dispatchEvent(new CustomEvent('voice-volume', {
              detail: { volume, maxVolume: 100, isActive: volume > SILENCE_THRESHOLD }
            }));
            
            // Debug volume levels
            if (volume > SILENCE_THRESHOLD) {
              console.log("üé§ VoiceController: Voice detected, volume:", volume);
            }
            
            if (volume > SILENCE_THRESHOLD) {
              silenceStart = Date.now();
            } else if (Date.now() - silenceStart > SILENCE_DURATION) {
              console.log("üé§ –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–µ—Ä–µ—Å—Ç–∞–≤ –≥–æ–≤–æ—Ä–∏—Ç–∏ - –∑—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó");

              // 4. –ó—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å –ø—Ä–∏ –≤–∏—è–≤–ª–µ–Ω–Ω—ñ —Ç–∏—à—ñ
              if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
                mediaRecorderRef.current.stop();
              }
              
              // Clean up microphone
              if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
                streamRef.current = null;
              }
              if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
                audioContextRef.current.close();
                audioContextRef.current = null;
              }
              cancelAnimationFrame(animationId);
              return;
            } else {
              // –¶–µ–π console.log –º–æ–∂–µ —Å–ø–∞–º–∏—Ç–∏, —Ç–æ–º—É —è –π–æ–≥–æ –ø—Ä–∏–±–µ—Ä—É
              // console.log("Silence detected, volume:", volume);
            }
            
            animationId = requestAnimationFrame(checkAudioLevel);
          } catch (error) {
            console.error("üé§ VoiceController: Error in audio level check:", error);
            // Clean up on error
            if (streamRef.current) {
              streamRef.current.getTracks().forEach(track => track.stop());
              streamRef.current = null;
            }
            if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
              audioContextRef.current.close();
              audioContextRef.current = null;
            }
            setIsListening(false);
            setShowAskButton(true);
            isMicButtonTriggered = false;
          }
        };
        
        // Store animation ID for cleanup
        animationId = requestAnimationFrame(checkAudioLevel);
        
        // Store cleanup function for potential early termination
        const cleanup = () => {
          if (animationId) {
            cancelAnimationFrame(animationId);
          }
          if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
          }
          if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close();
            audioContextRef.current = null;
          }
        };
        
        // Store cleanup function for potential use
        (window as any).voiceCleanup = cleanup;
        
      } catch (error) {
        console.error("üé§ VoiceController: Failed to access microphone:", error);
        const errorInfo = error instanceof Error ? {
          name: error.name,
          message: error.message,
          stack: error.stack
        } : { message: String(error) };
        
        console.log("üé§ VoiceController: Error details:", {
          ...errorInfo,
          userAgent: navigator.userAgent,
          protocol: window.location.protocol,
          hostname: window.location.hostname
        });
        
        // Enhanced deployment fallback
        const isDeployment = window.location.hostname.includes('.replit.app') || 
                           window.location.hostname !== 'localhost';
        
        if (isDeployment) {
          console.log("üé§ VoiceController: Deployment environment detected, using enhanced fallback");
        } else {
          console.log("üé§ VoiceController: Development environment, using standard fallback");
        }
        
        // Check if we should proceed with fallback
        if (!isListeningRef.current) {
          console.log("üé§ VoiceController: Listening stopped, aborting fallback");
          isMicButtonTriggered = false;
          return;
        }
        
        // Dispatch listening event for fallback
        console.log("üé§ VoiceController: Dispatching fallback mic-status listening event");
        window.dispatchEvent(new CustomEvent('mic-status', {
          detail: { status: 'listening' }
        }));
        
        // Simulate voice volume events during fallback listening
        let fallbackVolumeInterval: NodeJS.Timeout;
        let fallbackTimeout: NodeJS.Timeout;
        
        fallbackVolumeInterval = setInterval(() => {
          // Stop if listening state changed
          if (!isListeningRef.current) {
            clearInterval(fallbackVolumeInterval);
            return;
          }
          const volume = Math.random() * 40 + 20;
          window.dispatchEvent(new CustomEvent('voice-volume', {
            detail: { volume, maxVolume: 100, isActive: true }
          }));
        }, 150);
        
        // Enhanced fallback with deployment-specific timing
        const fallbackDelay = isDeployment ? 4000 : 3000; // Longer delay for deployment
        console.log(`üé§ VoiceController: Using fallback delay of ${fallbackDelay}ms for ${isDeployment ? 'deployment' : 'development'}`);
        
        fallbackTimeout = setTimeout(() => {
          clearInterval(fallbackVolumeInterval);
          
          // Check if still in listening state
          if (!isListeningRef.current) {
            console.log("üé§ VoiceController: Listening stopped during fallback, aborting");
            isMicButtonTriggered = false;
            return;
          }
          
          console.log("üé§ VoiceController: Fallback timer completed, transitioning to thinking");
          setIsListening(false);
          isListeningRef.current = false; // Update ref immediately
          setIsThinking(true);
          
          console.log("üé§ VoiceController: Dispatching mic-status processing event");
          window.dispatchEvent(new CustomEvent('mic-status', {
            detail: { status: 'processing' }
          }));
          
          // Enhanced thinking phase for deployment
          const thinkingDelay = isDeployment ? 2500 : 2000;
          console.log(`üé§ VoiceController: Using thinking delay of ${thinkingDelay}ms`);
          
          setTimeout(() => {
            if (!isListeningRef.current && isThinking) { // Only proceed if still in thinking state
              console.log("üé§ VoiceController: Thinking phase complete, starting response");
              setIsThinking(false);
              setIsResponding(true);
              setIsPlayingAudio(true);
              
              console.log("üé§ VoiceController: Dispatching mic-status stopped event");
              window.dispatchEvent(new CustomEvent('mic-status', {
                detail: { status: 'stopped' }
              }));
              
              handleVoiceResponse("Based on your question about this Ridge Zinfandel, I can tell you it's a bold wine with rich blackberry and spice notes, perfect for grilled meats and aged cheeses.");
              
              // Reset mic button flag after response
              setTimeout(() => {
                isMicButtonTriggered = false;
                console.log("üé§ VoiceController: Mic button flag reset complete");
              }, 1000);
            } else {
              console.log("üé§ VoiceController: Thinking phase interrupted, not proceeding to response");
              isMicButtonTriggered = false;
            }
          }, thinkingDelay);
        }, fallbackDelay);
        
        // Store cleanup for fallback timers
        (window as any).voiceFallbackCleanup = () => {
          clearInterval(fallbackVolumeInterval);
          clearTimeout(fallbackTimeout);
        };
      }
      
      // Reset mic button flag on error
      setTimeout(() => {
        isMicButtonTriggered = false;
      }, 8000);
    };

    const handleStopAudio = () => {
      console.log("üõë VoiceController: Stop button clicked - aborting all audio and listening");
      
      // Stop listening state immediately
      setIsListening(false);
      isListeningRef.current = false; // Update ref immediately
      setIsThinking(false);
      setIsResponding(false);
      setIsPlayingAudio(false);
      setShowUnmuteButton(false);
      setShowAskButton(true);
      
      // Clean up microphone streams and audio context
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      
      // Clean up animation frames and timers
      if ((window as any).voiceCleanup) {
        (window as any).voiceCleanup();
        (window as any).voiceCleanup = null;
      }
      if ((window as any).voiceFallbackCleanup) {
        (window as any).voiceFallbackCleanup();
        (window as any).voiceFallbackCleanup = null;
      }
      
      // Abort ongoing TTS request
      if (currentTTSRequestRef.current) {
        console.log("üõë VoiceController: Aborting TTS request");
        currentTTSRequestRef.current.abort();
        currentTTSRequestRef.current = null;
      }
      
      // Stop audio playback
      if (currentAudioRef.current) {
        console.log("üõë VoiceController: Stopping audio playback");
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current = null;
      }
      
      // Reset flags
      isMicButtonTriggered = false;
      isVoiceButtonTriggered = false;
      
      // Dispatch stop event for circle animation
      window.dispatchEvent(new CustomEvent('mic-status', {
        detail: { status: 'stopped' }
      }));
      
      // Call original stop function
      stopAudio();
      
      console.log("üõë VoiceController: Stop button processing complete");
    };

    const handleDeploymentAudioStopped = () => {
      setIsPlayingAudio(false);
      setIsResponding(false);
      setShowUnmuteButton(false);
      setShowAskButton(true);
      currentAudioRef.current = null;
    };

    window.addEventListener('triggerVoiceAssistant', handleTriggerVoiceAssistant);
    window.addEventListener('triggerMicButton', handleTriggerMicButton);
    window.addEventListener('stopVoiceAudio', handleStopAudio);
    window.addEventListener('deploymentAudioStopped', handleDeploymentAudioStopped);

    return () => {
      window.removeEventListener('triggerVoiceAssistant', handleTriggerVoiceAssistant);
      window.removeEventListener('triggerMicButton', handleTriggerMicButton);
      window.removeEventListener('stopVoiceAudio', handleStopAudio);
      window.removeEventListener('deploymentAudioStopped', handleDeploymentAudioStopped);
    };
  }, [isListening]);

  const handleVoiceResponse = async (responseText: string) => {
    try {
      // Create AbortController for this TTS request
      const abortController = new AbortController();
      currentTTSRequestRef.current = abortController;
      
      console.log("üé§ VoiceController: Starting TTS request with abort controller");
      
      const response = await fetch('/api/text-to-speech', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text: responseText }),
        signal: abortController.signal,
      });

      if (!response.ok) {
        throw new Error('Failed to generate speech');
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      currentAudioRef.current = audio;

      console.log("üé§ VoiceController: Starting audio playback");
      await audio.play();

      audio.onended = () => {
        console.log("üé§ VoiceController: Audio playback ended naturally");
        setIsPlayingAudio(false);
        setIsResponding(false);
        setShowUnmuteButton(false);
        setShowAskButton(true);
        currentAudioRef.current = null;
        currentTTSRequestRef.current = null;
        URL.revokeObjectURL(audioUrl);
      };

      if (responseText && onSendMessage) {
        onSendMessage(responseText.trim());
      }

    } catch (error: any) {
      if (error?.name === 'AbortError') {
        console.log("üõë VoiceController: TTS request was aborted by stop button");
      } else {
        console.error('Error in voice response:', error);
      }
      setIsPlayingAudio(false);
      setIsResponding(false);
      setShowUnmuteButton(false);
      setShowAskButton(true);
      currentTTSRequestRef.current = null;
    }
  };

  const handleClose = () => {
    setShowBottomSheet(false);
    setIsListening(false);
    setIsResponding(false);
    setIsThinking(false);
    setIsPlayingAudio(false);
    setShowUnmuteButton(false);
    setShowAskButton(false);
    isManuallyClosedRef.current = true;
    stopRecording();
    setTimeout(() => {
      isManuallyClosedRef.current = false;
    }, 1000);
  };

  const handleAskRecording = () => {
    startRecording();
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      // --- –î–æ–¥–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–∏—à—ñ ---
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);
      analyserRef.current = analyser;
      const dataArray = new Uint8Array(analyser.fftSize);

      // --- –§—É–Ω–∫—Ü—ñ—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ç–∏—à—ñ ---
      const checkSilence = () => {
        analyser.getByteTimeDomainData(dataArray);
        // –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—å–æ—ó –∞–º–ø–ª—ñ—Ç—É–¥–∏
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += Math.abs(dataArray[i] - 128);
        }
        const avg = sum / dataArray.length;

        if (avg < SILENCE_THRESHOLD) {
          if (!silenceStartRef.current) silenceStartRef.current = Date.now();
          if (Date.now() - (silenceStartRef.current || 0) > SILENCE_DURATION) {
            // –¢–∏—à–∞ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–æ–≤–≥–æ ‚Äî –∑—É–ø–∏–Ω—è—î–º–æ –∑–∞–ø–∏—Å
            if (mediaRecorder.state === "recording") {
              mediaRecorder.stop();
            }
            stream.getTracks().forEach(track => track.stop());
            audioContext.close();
            return;
          }
        } else {
          silenceStartRef.current = null;
        }
        // –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É
        silenceTimeoutRef.current = setTimeout(checkSilence, 100);
      };

      // --- –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —Ç–∏—à—ñ ---
      checkSilence();

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        // –û—á–∏—â–∞—î–º–æ —Ç–∞–π–º–µ—Ä–∏ —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏
        if (silenceTimeoutRef.current) clearTimeout(silenceTimeoutRef.current);
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
          audioContextRef.current.close();
          audioContextRef.current = null;
        }
        analyserRef.current = null;
        silenceStartRef.current = null;

        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        const formData = new FormData();
        formData.append('audio', audioBlob);

        setIsThinking(true);
        setIsListening(false);

        try {
          const response = await fetch('/api/transcribe', {
            method: 'POST',
            body: formData,
          });
          const result = await response.json();
          if (result.text && onSendMessage) {
            onSendMessage(result.text.trim());
          }
        } catch (err) {
          console.error("Transcribe error:", err);
        } finally {
          setIsThinking(false);
          setIsResponding(false);
        }
      };

      mediaRecorder.start();
      setIsListening(true);
      setIsResponding(false);
      setIsThinking(false);
      setIsPlayingAudio(false);
      setShowUnmuteButton(false);
      setShowAskButton(false);

      // --- (–û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ) –ê–≤—Ç–æ—Å—Ç–æ–ø —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥ —è–∫ –∑–∞–ø–∞—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç ---
      setTimeout(() => {
        if (mediaRecorder.state === "recording") {
          mediaRecorder.stop();
        }
        stream.getTracks().forEach(track => track.stop());
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') audioContextRef.current.close();
      }, 10000);

    } catch (error) {
      console.error("Failed to start recording:", error);
      setIsListening(false);
      setIsThinking(false);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.stop();
    }
  };

  return (
    <VoiceAssistantBottomSheet
      isOpen={showBottomSheet}
      onClose={handleClose}
      onMute={stopRecording}
      onAsk={handleAskRecording}
      isListening={isListening}
      isResponding={isResponding}
      isThinking={isThinking}
      isPlayingAudio={isPlayingAudio}
      showUnmuteButton={showUnmuteButton}
      showAskButton={showAskButton}
      showSuggestions={true}
      onStopAudio={stopRecording}
      onUnmute={stopRecording}
      wineKey={wineKey}
      onSuggestionClick={(suggestion: string, pillId?: string, options?: { textOnly?: boolean; instantResponse?: string }) => {
        if (onSendMessage) {
          onSendMessage(suggestion);
        }
      }}
    />
  );
};

export default VoiceController;